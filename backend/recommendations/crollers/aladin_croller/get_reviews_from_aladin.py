# ================================
#  0. ë¼ì´ë¸ŒëŸ¬ë¦¬ import
# ================================

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from tqdm import tqdm


# ================================
#  1. ê¸°ë³¸ ì„¤ì •
# ================================

crolling_platform = 'aladin'
crolling_data_type = 'reviews'

## TODO csvê°€ ì•„ë‹Œ, ë™ê¸° ì½”ë“œ(books.views.resolve)ì—ì„œ ì €ì¥í•  ë„ì„œì˜ ì •ë³´ë¥¼ ë°›ì•„ì™€ INPUT ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
INPUT_CSV = f"final_books_top204.csv"   #  ì…ë ¥ csv

OUTPUT_CSV = f"{crolling_platform}_{crolling_data_type}.csv"      #  ìµœì¢… ë¦¬ë·° ì €ì¥ CSV
#ì €ì¥ í˜•ì‹: isbn13,source,review_text,rating,review_date,blog_url

HEADERS = {
    "User-Agent": "Mozilla/5.0"
}
#  ì•Œë¼ë”˜ì€ User-Agent ì—†ìœ¼ë©´ ì‘ë‹µì´ ë¶ˆì•ˆì •í•œ ê²½ìš° ìˆìŒ


# ================================
#  2. CSV ë¡œë“œ
# ================================

books_df = pd.read_csv(INPUT_CSV)
print(f"âœ… ì´ {len(books_df)}ê¶Œ ë¡œë“œ ì™„ë£Œ")


# ================================
# 3. ISBN â†’ ì•Œë¼ë”˜ ItemId ì¶”ì¶œ
# ================================

def get_aladin_item_id(isbn):
    """
    ISBNìœ¼ë¡œ ì•Œë¼ë”˜ ê²€ìƒ‰ í›„,
    div.ss_book_box ë‚´ë¶€ì˜ ì²« ë²ˆì§¸ ë„ì„œ ItemId ì¶”ì¶œ
    """

    search_url = (
        "https://www.aladin.co.kr/search/wsearchresult.aspx"
        f"?SearchTarget=Book&SearchWord={isbn}"
    )
    res = requests.get(search_url, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")

    book_boxes = soup.select("div.ss_book_box")

    if not book_boxes:
        print("ss_book_box ì—†ìŒ â†’ ê²€ìƒ‰ ì‹¤íŒ¨")
        return None

    # ì²« ë²ˆì§¸ ì±… ë°•ìŠ¤ë§Œ ì„ íƒ
    box = book_boxes[0]

    # ê·¸ ì•ˆì—ì„œ wproduct.aspx ë§í¬ ì°¾ê¸°
    link = box.select_one("a[href*='wproduct.aspx?ItemId=']")

    if not link:
        print("âŒ ss_book_box ì•ˆì— ItemId ë§í¬ ì—†ìŒ")
        return None
    
    href = link.get("href")
    item_id = href.split("ItemId=")[-1].split("&")[0]

    return item_id



# ================================
#  4. 100ìí‰ í¬ë¡¤ë§
# ================================

def crawl_short_reviews(item_id, isbn, max_pages=1):

    reviews = []

    for page in range(1, max_pages + 1):

        url = (
            "https://www.aladin.co.kr/ucl/shop/product/ajax/GetCommunityListAjax.aspx"
            f"?ProductItemId={item_id}"
            f"&itemId={item_id}"
            f"&pageCount={max_pages}"
            "&communitytype=CommentReview"
            "&nemoType=-1"
            f"&page={page}"
            "&startNumber=1"
            "&endNumber=10"
            "&sort=2"
            "&IsOrderer=1"
            "&BranchType=1"
            "&IsAjax=true"
            "&pageType=0"
        )

        headers = {
            "User-Agent": "Mozilla/5.0",
            "Referer": f"https://www.aladin.co.kr/shop/wproduct.aspx?ItemId={item_id}",
            "X-Requested-With": "XMLHttpRequest"
        }
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")

        li_blocks = soup.select("li")

        if not li_blocks:
            print("âš ï¸ 100ìí‰ AJAX ì‘ë‹µ ì—†ìŒ â†’ ì¤‘ë‹¨")
            break

        #  2ê°œ liê°€ í•œ ì„¸íŠ¸ì´ë¯€ë¡œ 2ì¹¸ì”© ì í”„
        for i in range(0, len(li_blocks), 2):
            
            try:
                
                content_li = li_blocks[i]
                meta_li = li_blocks[i + 1]

                #  ë¦¬ë·° ë³¸ë¬¸ (ìŠ¤í¬ì¼ëŸ¬ ì œì™¸)
                text_tag = content_li.select_one(
                    "span[id^='spnPaper']:not([id*='Spoiler'])"
                )
                review_text = text_tag.text.strip() if text_tag else None

                #  ë¸”ë¡œê·¸ ë§í¬
                blog_tag = content_li.select_one("a[href*='blog.aladin.co.kr']")
                blog_url = blog_tag["href"] if blog_tag else None

                # â­ ë³„ì  (img ê°œìˆ˜ ê¸°ë°˜, div.hundred_list ê¸°ì¤€)
                hundred_box = content_li.find_parent("div", class_="hundred_list")

                if hundred_box:
                    star_tag = hundred_box.select_one("div.HL_star")

                    if star_tag:
                        star_imgs = star_tag.find_all("img")
                        star_on_count = sum(
                            1 for img in star_imgs 
                            if "icon_star_on" in img.get("src", "")
                        )
                        rating = star_on_count * 2   # âœ… 10ì  í™˜ì‚°
                    else:
                        rating = None
                else:
                    rating = None
                

                #  ë‚ ì§œ
                date_tag = meta_li.select_one("div.left span")
                review_date = date_tag.text.strip() if date_tag else None

                if review_text:
                    reviews.append({
                        "isbn13": isbn,
                        "source": "aladin_short",
                        "review_text": review_text,
                        "rating": rating,
                        "review_date": review_date,
                        "blog_url": blog_url
                    })

            except IndexError:
                continue   # âœ… í™€ìˆ˜ ê¹¨ì§ˆ ë•Œ ì•ˆì „ ì²˜ë¦¬

        time.sleep(1.5)

    return reviews

# ================================
#  5. ë§ˆì´ë¦¬ë·° í¬ë¡¤ë§
# ================================

def crawl_my_reviews(isbn, max_pages=50):

    reviews = []

    for page in range(1, max_pages + 1):

        url = (
            "https://www.aladin.co.kr/shop/product/getContents.aspx"
            f"?ISBN={isbn}"
            "&name=MyReview"
            "&type=0"
            f"&page={page}"
        )
        print(url)
        headers = {
            "User-Agent": "Mozilla/5.0",
            "Referer": "https://www.aladin.co.kr",
            "X-Requested-With": "XMLHttpRequest"
        }

        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")

        review_boxes = soup.select("div.hundred_list")

        if not review_boxes:
            print("âš ï¸ ë§ˆì´ë¦¬ë·° ë” ì´ìƒ ì—†ìŒ â†’ ì¤‘ë‹¨")
            break

        for box in review_boxes:
            # ë³„ì  ê³„ì‚°
            star_tag = box.select_one("div.HL_star")

            if star_tag:
                star_imgs = star_tag.find_all("img")
                
                # on / off ìƒê´€ì—†ì´ img ê°œìˆ˜ë¡œ ì„¸ëŠ” ë°©ì‹
                star_on_count = sum(
                    1 for img in star_imgs 
                    if "icon_star_on" in img.get("src", "")
                )

                rating = star_on_count * 2   # 10ì  ë§Œì ìœ¼ë¡œ ë³„ì  í™˜ì‚°
            else:
                rating = None

            #  ì§§ì€ ë¦¬ë·°
            short_tag = box.select_one("div[id^='divPaper']")
            review_text = short_tag.text.strip() if short_tag else None

            #  ì „ì²´ ë¦¬ë·° (ë”ë³´ê¸° ëˆŒë €ì„ ë•Œ ë‚˜ì˜¤ëŠ” ê²ƒ)
            full_tag = box.select_one("div[id^='paperAll_']")
            if full_tag and full_tag.text.strip():
                review_text = full_tag.text.strip()

            if review_text:
                reviews.append({
                    "isbn13": isbn,
                    "source": "aladin_myreview",
                    "review_text": review_text,
                    "rating": rating,
                    "review_date": None,
                    "blog_url": None
                })

        time.sleep(1.2)

    return reviews





# ================================
# 6. ì „ì²´ ISBN ìˆœíšŒ
# ================================

all_reviews = []

print(" ì•Œë¼ë”˜ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘")

for idx, row in tqdm(books_df.iterrows(), total=len(books_df)):
    isbn = str(row["isbn13"]).strip()
    title = row["title"]

    print(f"\nğŸ“˜ í¬ë¡¤ë§ ì¤‘: {title} ({isbn})")

    # 1ë‹¨ê³„: ItemId ì¶”ì¶œ
    item_id = get_aladin_item_id(isbn)

    if not item_id:
        print("âŒ ItemId ëª» ì°¾ìŒ â†’ ìŠ¤í‚µ")
        continue

    print(f"ItemId ì¶”ì¶œ ì„±ê³µ: {item_id}")


    # 2ë‹¨ê³„: 100ìí‰
    short_reviews = crawl_short_reviews(item_id, isbn, max_pages=50)
    print(f"100ìí‰ {len(short_reviews)}ê°œ")

    """# 3ë‹¨ê³„: ë§ˆì´ë¦¬ë·°
    #my_reviews = crawl_my_reviews(isbn, max_pages=10)
    my_reviews = new_crawl_my_reviews(item_id, max_pages=10)
    print(f"ë§ˆì´ë¦¬ë·° {len(my_reviews)}ê°œ")"""



    all_reviews.extend(short_reviews)
    #all_reviews.extend(my_reviews)

    # ISBN í•˜ë‚˜ ì²˜ë¦¬ í›„ ì‰¬ê¸°
    time.sleep(1)


# ================================
# 7. CSV ì €ì¥
# ================================

reviews_df = pd.DataFrame(all_reviews)

reviews_df.to_csv(
    OUTPUT_CSV,
    index=False,
    encoding="utf-8-sig"
)

print("ì•Œë¼ë”˜ ë¦¬ë·° í¬ë¡¤ë§ ì™„ë£Œ!")
print(f"ì €ì¥ íŒŒì¼: {OUTPUT_CSV}")
